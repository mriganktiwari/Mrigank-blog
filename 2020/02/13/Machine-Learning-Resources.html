<p><em>I am currently working on audio/speech processing, TTS etc., so…</em></p>

<h3 id="deep-learning-in-speech-resources">Deep Learning in Speech resources:</h3>
<ul>
  <li>Understanding sound/audio:
    <ul>
      <li>@<a href="https://twitter.com/radekosmulski">radekosmulski</a> - <a href="https://github.com/earthspecies/from_zero_to_DSP/">A great starting point</a>, also usable in Google Colab.</li>
      <li><a href="https://github.com/mogwai/fastai_audio/blob/master/tutorials/01_Intro_to_Audio.ipynb/">Very good intro to audio</a>.</li>
      <li><a href="https://github.com/sebastianruder/NLP-progress">Progress tracker</a> in NLP by @<a href="https://twitter.com/seb_ruder">seb_ruder</a></li>
      <li><a href="https://waitbutwhy.com/2016/03/sound.html">Everything you should know about sound</a></li>
    </ul>
  </li>
  <li>
    <p>Generation/Synthesis of new sounds based on training set (<strong><a href="https://github.com/drscotthawley/drscotthawley.github.io/blob/master/_posts/2017-2-6-Machine-Learning-Reference-List.md">copied from</a></strong>):</p>

    <ul>
      <li>Jake Fiala: “Deep Learning and Sound” <a href="http://fiala.uk/notes/deep-learning-and-sound-01-intro">http://fiala.uk/notes/deep-learning-and-sound-01-intro</a></li>
      <li>GRUV: <a href="https://github.com/MattVitelli/GRUV">https://github.com/MattVitelli/GRUV</a>.  Btw, found that LSTM worked better than GRU.</li>
      <li>John Glover: <a href="http://www.johnglover.net/blog/generating-sound-with-rnns.html">http://www.johnglover.net/blog/generating-sound-with-rnns.html</a>  Glover used LSTM fed by phase vocoder (really just STFT).</li>
      <li>Google Magenta for MIDI: <a href="https://magenta.tensorflow.org/welcome-to-magenta">https://magenta.tensorflow.org/welcome-to-magenta</a></li>
      <li>Google WaveNet for Audio… <a href="https://deepmind.com/blog/wavenet-generative-model-raw-audio/">https://deepmind.com/blog/wavenet-generative-model-raw-audio/</a>
        <ul>
          <li>WaveNet is slow.   “Fast Wavenet”: <a href="https://github.com/tomlepaine/fast-wavenet">https://github.com/tomlepaine/fast-wavenet</a></li>
          <li>WaveNet in Keras: <a href="https://github.com/basveeling/wavenet">https://github.com/basveeling/wavenet</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Neural voice cloning with few samples implementations (to be added):
    <ul>
      <li>Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis - <a href="https://arxiv.org/abs/1806.04558">paper</a>
        <ul>
          <li><a href="https://github.com/CorentinJ/Real-Time-Voice-Cloning">Github implementation</a></li>
        </ul>
      </li>
      <li>Neural Voice Cloning with a Few Samples - <a href="https://arxiv.org/abs/1802.06006">paper</a>
        <ul>
          <li><a href="https://github.com/Sharad24/Neural-Voice-Cloning-with-Few-Samples">Github implementation</a></li>
        </ul>
      </li>
      <li>DeepVoice3 - <a href="https://arxiv.org/abs/1710.07654">paper</a></li>
      <li>Wavenet - <a href="https://deepmind.com/blog/article/wavenet-generative-model-raw-audio">blog</a></li>
    </ul>
  </li>
  <li>Text-to-speech implementations (to be added):
    <ul>
      <li>Tacotron2 - <a href="https://arxiv.org/abs/1712.05884">paper</a></li>
      <li>DCTTS - <a href="https://arxiv.org/pdf/1901.04276.pdf">paper</a></li>
    </ul>
  </li>
</ul>

<h3 id="deep-learning-cheatsheet-by-rusty1s"><a href="https://github.com/rusty1s/deep-learning-cheatsheet"><strong>Deep Learning cheatsheet</strong></a> by @<a href="https://twitter.com/rusty1s">rusty1s</a>.</h3>

<h3 id="resources-by-thom_wolf">Resources by @<a href="https://twitter.com/Thom_Wolf">thom_wolf</a>.</h3>

<p><em>My self-educational approach is usually to get a few rather exhaustive books and read them for cover to cover.</em></p>
<ul>
  <li>Here is my reading list to join the NLP/AI/ML field.
    <ul>
      <li><a href="https://www.deeplearningbook.org/">The “Deep Learning” Book</a> by Ian Goodfellow, Yoshua Bengio and Aaron Courville is a good ressource to get a quick overview of the current tools.</li>
      <li><a href="http://aima.cs.berkeley.edu/">“Artificial Intelligence: A Modern Approach”</a> by Stuart Russell and Peter Norvig is a great ressource for all pre-neural-network tools and methods.</li>
      <li><a href="https://www.cs.ubc.ca/~murphyk/MLbook/">“Machine Learning: A Probabilistic Perspective”</a> by Kevin P. Murphy is a great ressource to go deeper in the probabilistic approach and get a good exposure to Bayesian tools.</li>
      <li><a href="http://www.inference.org.uk/mackay/itila/book.html">“Information Theory, Inference and Learning Algorithms”</a> by David MacKay is a little gem that explain propabilities and Information theory so clearly it’s almost unbelievable.</li>
      <li><strong>“The Book of Why: The New Science of Cause and Effect”</strong> by Pearl, Judea is a good introduction to Causality (more accessible than the big “Causality: Models, Reasoning and Inference”)</li>
      <li><a href="http://incompleteideas.net/book/the-book.html">“Reinforcement Learning: An Introduction”</a> by Richard S. Sutton and Andrew G. Barto is a great ressource to get an introductory exposure to Reinforcement Learning</li>
    </ul>
  </li>
  <li>Natural Language Processing: three great ressources I’ve read with interest:
    <ul>
      <li><a href="https://github.com/nyu-dl/NLP_DL_Lecture_Note/blob/master/lecture_note.pdf">Kyunghyun Cho’s lecture notes</a> on “Natural Language Processing with Representation Learning” are great.</li>
      <li><a href="https://www.amazon.com/Language-Processing-Synthesis-Lectures-Technologies/dp/1627052984">Yoav Goldberg’s book on “Neural Network Methods in Natural Language Processing”</a> is nice too (see also an older free version <a href="https://arxiv.org/abs/1510.00726">here</a>).</li>
      <li><a href="https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf">Jacob Eisenstein’s textbook on “Natural Language Processing”</a> is also a very exhaustive read.</li>
    </ul>
  </li>
  <li>It’s also good to complement this with a few online courses depending on what field you feel you should be diving deeper into.
I took the following classes:
    <ul>
      <li><a href="https://courses.edx.org/courses/course-v1:MITx+6.008.1x+3T2016/course/">Computational Probability and Inference (6.008.1x)</a> from edx.</li>
      <li><a href="https://www.coursera.org/specializations/probabilistic-graphical-models">Probabilistic Graphical Models Specialization</a> from coursera.</li>
    </ul>
  </li>
</ul>
